{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a CNN for image recognition.\n",
    "\n",
    "### Name: Zhili Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\nshape of y_train: (50000, 1)\nshape of x_test: (10000, 32, 32, 3)\nshape of y_test: (10000, 1)\nnumber of classes: 10\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "mean = numpy.mean(x_train,axis=(0,1,2,3))\n",
    "std = numpy.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Shape of y_train_vec: (50000, 10)\nShape of y_test_vec: (10000, 10)\n[6]\n[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    temp = numpy.zeros(shape=(y.shape[0],num_class))\n",
    "    for i in range(y.shape[0]):\n",
    "        temp[i,y[i]] = 1\n",
    "    return temp\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\nShape of y_tr: (40000, 10)\nShape of x_val: (10000, 32, 32, 3)\nShape of y_val: (10000, 10)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"model_18\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_19 (InputLayer)           (None, 32, 32, 3)    0                                            \n__________________________________________________________________________________________________\ninput_layor (Conv2D)            (None, 32, 32, 32)   896         input_19[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_131 (BatchN (None, 32, 32, 32)   128         input_layor[0][0]                \n__________________________________________________________________________________________________\nactivation_654 (Activation)     (None, 32, 32, 32)   0           batch_normalization_131[0][0]    \n__________________________________________________________________________________________________\nconv2d_313 (Conv2D)             (None, 32, 32, 32)   9248        activation_654[0][0]             \n__________________________________________________________________________________________________\nactivation_655 (Activation)     (None, 32, 32, 32)   0           conv2d_313[0][0]                 \n__________________________________________________________________________________________________\nadd_257 (Add)                   (None, 32, 32, 32)   0           activation_655[0][0]             \n                                                                 activation_654[0][0]             \n__________________________________________________________________________________________________\nactivation_656 (Activation)     (None, 32, 32, 32)   0           add_257[0][0]                    \n__________________________________________________________________________________________________\nconv2d_314 (Conv2D)             (None, 32, 32, 32)   9248        activation_656[0][0]             \n__________________________________________________________________________________________________\nactivation_657 (Activation)     (None, 32, 32, 32)   0           conv2d_314[0][0]                 \n__________________________________________________________________________________________________\nadd_258 (Add)                   (None, 32, 32, 32)   0           activation_657[0][0]             \n                                                                 activation_656[0][0]             \n__________________________________________________________________________________________________\nactivation_658 (Activation)     (None, 32, 32, 32)   0           add_258[0][0]                    \n__________________________________________________________________________________________________\nconv2d_315 (Conv2D)             (None, 32, 32, 32)   9248        activation_658[0][0]             \n__________________________________________________________________________________________________\nactivation_659 (Activation)     (None, 32, 32, 32)   0           conv2d_315[0][0]                 \n__________________________________________________________________________________________________\nadd_259 (Add)                   (None, 32, 32, 32)   0           activation_659[0][0]             \n                                                                 activation_658[0][0]             \n__________________________________________________________________________________________________\nactivation_660 (Activation)     (None, 32, 32, 32)   0           add_259[0][0]                    \n__________________________________________________________________________________________________\nconv2d_316 (Conv2D)             (None, 32, 32, 32)   9248        activation_660[0][0]             \n__________________________________________________________________________________________________\nactivation_661 (Activation)     (None, 32, 32, 32)   0           conv2d_316[0][0]                 \n__________________________________________________________________________________________________\nadd_260 (Add)                   (None, 32, 32, 32)   0           activation_661[0][0]             \n                                                                 activation_660[0][0]             \n__________________________________________________________________________________________________\nactivation_662 (Activation)     (None, 32, 32, 32)   0           add_260[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling2d_36 (MaxPooling2D) (None, 16, 16, 32)   0           activation_662[0][0]             \n__________________________________________________________________________________________________\ndropout_197 (Dropout)           (None, 16, 16, 32)   0           max_pooling2d_36[0][0]           \n__________________________________________________________________________________________________\nconv2d_318 (Conv2D)             (None, 16, 16, 64)   18496       dropout_197[0][0]                \n__________________________________________________________________________________________________\nconv2d_317 (Conv2D)             (None, 16, 16, 64)   18496       max_pooling2d_36[0][0]           \n__________________________________________________________________________________________________\nactivation_664 (Activation)     (None, 16, 16, 64)   0           conv2d_318[0][0]                 \n__________________________________________________________________________________________________\nactivation_663 (Activation)     (None, 16, 16, 64)   0           conv2d_317[0][0]                 \n__________________________________________________________________________________________________\nadd_261 (Add)                   (None, 16, 16, 64)   0           activation_664[0][0]             \n                                                                 activation_663[0][0]             \n__________________________________________________________________________________________________\nactivation_665 (Activation)     (None, 16, 16, 64)   0           add_261[0][0]                    \n__________________________________________________________________________________________________\ndropout_198 (Dropout)           (None, 16, 16, 64)   0           activation_665[0][0]             \n__________________________________________________________________________________________________\nconv2d_319 (Conv2D)             (None, 16, 16, 64)   36928       dropout_198[0][0]                \n__________________________________________________________________________________________________\nactivation_666 (Activation)     (None, 16, 16, 64)   0           conv2d_319[0][0]                 \n__________________________________________________________________________________________________\nadd_262 (Add)                   (None, 16, 16, 64)   0           activation_666[0][0]             \n                                                                 activation_665[0][0]             \n__________________________________________________________________________________________________\nactivation_667 (Activation)     (None, 16, 16, 64)   0           add_262[0][0]                    \n__________________________________________________________________________________________________\ndropout_199 (Dropout)           (None, 16, 16, 64)   0           activation_667[0][0]             \n__________________________________________________________________________________________________\nconv2d_320 (Conv2D)             (None, 16, 16, 64)   36928       dropout_199[0][0]                \n__________________________________________________________________________________________________\nactivation_668 (Activation)     (None, 16, 16, 64)   0           conv2d_320[0][0]                 \n__________________________________________________________________________________________________\nadd_263 (Add)                   (None, 16, 16, 64)   0           activation_668[0][0]             \n                                                                 activation_667[0][0]             \n__________________________________________________________________________________________________\nactivation_669 (Activation)     (None, 16, 16, 64)   0           add_263[0][0]                    \n__________________________________________________________________________________________________\ndropout_200 (Dropout)           (None, 16, 16, 64)   0           activation_669[0][0]             \n__________________________________________________________________________________________________\nconv2d_321 (Conv2D)             (None, 16, 16, 64)   36928       dropout_200[0][0]                \n__________________________________________________________________________________________________\nactivation_670 (Activation)     (None, 16, 16, 64)   0           conv2d_321[0][0]                 \n__________________________________________________________________________________________________\nadd_264 (Add)                   (None, 16, 16, 64)   0           activation_670[0][0]             \n                                                                 activation_669[0][0]             \n__________________________________________________________________________________________________\nactivation_671 (Activation)     (None, 16, 16, 64)   0           add_264[0][0]                    \n__________________________________________________________________________________________________\ndropout_201 (Dropout)           (None, 16, 16, 64)   0           activation_671[0][0]             \n__________________________________________________________________________________________________\nconv2d_322 (Conv2D)             (None, 16, 16, 64)   36928       dropout_201[0][0]                \n__________________________________________________________________________________________________\nactivation_672 (Activation)     (None, 16, 16, 64)   0           conv2d_322[0][0]                 \n__________________________________________________________________________________________________\nadd_265 (Add)                   (None, 16, 16, 64)   0           activation_672[0][0]             \n                                                                 activation_671[0][0]             \n__________________________________________________________________________________________________\nactivation_673 (Activation)     (None, 16, 16, 64)   0           add_265[0][0]                    \n__________________________________________________________________________________________________\ndropout_202 (Dropout)           (None, 16, 16, 64)   0           activation_673[0][0]             \n__________________________________________________________________________________________________\nconv2d_323 (Conv2D)             (None, 16, 16, 64)   36928       dropout_202[0][0]                \n__________________________________________________________________________________________________\nactivation_674 (Activation)     (None, 16, 16, 64)   0           conv2d_323[0][0]                 \n__________________________________________________________________________________________________\nadd_266 (Add)                   (None, 16, 16, 64)   0           activation_674[0][0]             \n                                                                 activation_673[0][0]             \n__________________________________________________________________________________________________\nactivation_675 (Activation)     (None, 16, 16, 64)   0           add_266[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling2d_37 (MaxPooling2D) (None, 8, 8, 64)     0           activation_675[0][0]             \n__________________________________________________________________________________________________\ndropout_205 (Dropout)           (None, 8, 8, 64)     0           max_pooling2d_37[0][0]           \n__________________________________________________________________________________________________\nconv2d_325 (Conv2D)             (None, 8, 8, 128)    73856       dropout_205[0][0]                \n__________________________________________________________________________________________________\nconv2d_324 (Conv2D)             (None, 8, 8, 128)    73856       max_pooling2d_37[0][0]           \n__________________________________________________________________________________________________\nbatch_normalization_133 (BatchN (None, 8, 8, 128)    512         conv2d_325[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_132 (BatchN (None, 8, 8, 128)    512         conv2d_324[0][0]                 \n__________________________________________________________________________________________________\nactivation_679 (Activation)     (None, 8, 8, 128)    0           batch_normalization_133[0][0]    \n__________________________________________________________________________________________________\nactivation_678 (Activation)     (None, 8, 8, 128)    0           batch_normalization_132[0][0]    \n__________________________________________________________________________________________________\nadd_267 (Add)                   (None, 8, 8, 128)    0           activation_679[0][0]             \n                                                                 activation_678[0][0]             \n__________________________________________________________________________________________________\nactivation_680 (Activation)     (None, 8, 8, 128)    0           add_267[0][0]                    \n__________________________________________________________________________________________________\ndropout_206 (Dropout)           (None, 8, 8, 128)    0           activation_680[0][0]             \n__________________________________________________________________________________________________\nconv2d_326 (Conv2D)             (None, 8, 8, 128)    147584      dropout_206[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_134 (BatchN (None, 8, 8, 128)    512         conv2d_326[0][0]                 \n__________________________________________________________________________________________________\nactivation_681 (Activation)     (None, 8, 8, 128)    0           batch_normalization_134[0][0]    \n__________________________________________________________________________________________________\nadd_268 (Add)                   (None, 8, 8, 128)    0           activation_681[0][0]             \n                                                                 activation_680[0][0]             \n__________________________________________________________________________________________________\nactivation_682 (Activation)     (None, 8, 8, 128)    0           add_268[0][0]                    \n__________________________________________________________________________________________________\ndropout_207 (Dropout)           (None, 8, 8, 128)    0           activation_682[0][0]             \n__________________________________________________________________________________________________\nconv2d_327 (Conv2D)             (None, 8, 8, 128)    147584      dropout_207[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_135 (BatchN (None, 8, 8, 128)    512         conv2d_327[0][0]                 \n__________________________________________________________________________________________________\nactivation_683 (Activation)     (None, 8, 8, 128)    0           batch_normalization_135[0][0]    \n__________________________________________________________________________________________________\nadd_269 (Add)                   (None, 8, 8, 128)    0           activation_683[0][0]             \n                                                                 activation_682[0][0]             \n__________________________________________________________________________________________________\nactivation_684 (Activation)     (None, 8, 8, 128)    0           add_269[0][0]                    \n__________________________________________________________________________________________________\ndropout_208 (Dropout)           (None, 8, 8, 128)    0           activation_684[0][0]             \n__________________________________________________________________________________________________\nconv2d_328 (Conv2D)             (None, 8, 8, 128)    147584      dropout_208[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_136 (BatchN (None, 8, 8, 128)    512         conv2d_328[0][0]                 \n__________________________________________________________________________________________________\nactivation_685 (Activation)     (None, 8, 8, 128)    0           batch_normalization_136[0][0]    \n__________________________________________________________________________________________________\nadd_270 (Add)                   (None, 8, 8, 128)    0           activation_685[0][0]             \n                                                                 activation_684[0][0]             \n__________________________________________________________________________________________________\nactivation_686 (Activation)     (None, 8, 8, 128)    0           add_270[0][0]                    \n__________________________________________________________________________________________________\ndropout_209 (Dropout)           (None, 8, 8, 128)    0           activation_686[0][0]             \n__________________________________________________________________________________________________\nconv2d_329 (Conv2D)             (None, 8, 8, 128)    147584      dropout_209[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_137 (BatchN (None, 8, 8, 128)    512         conv2d_329[0][0]                 \n__________________________________________________________________________________________________\nactivation_687 (Activation)     (None, 8, 8, 128)    0           batch_normalization_137[0][0]    \n__________________________________________________________________________________________________\nadd_271 (Add)                   (None, 8, 8, 128)    0           activation_687[0][0]             \n                                                                 activation_686[0][0]             \n__________________________________________________________________________________________________\nactivation_688 (Activation)     (None, 8, 8, 128)    0           add_271[0][0]                    \n__________________________________________________________________________________________________\ndropout_210 (Dropout)           (None, 8, 8, 128)    0           activation_688[0][0]             \n__________________________________________________________________________________________________\nconv2d_330 (Conv2D)             (None, 8, 8, 128)    147584      dropout_210[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_138 (BatchN (None, 8, 8, 128)    512         conv2d_330[0][0]                 \n__________________________________________________________________________________________________\nactivation_689 (Activation)     (None, 8, 8, 128)    0           batch_normalization_138[0][0]    \n__________________________________________________________________________________________________\nadd_272 (Add)                   (None, 8, 8, 128)    0           activation_689[0][0]             \n                                                                 activation_688[0][0]             \n__________________________________________________________________________________________________\nactivation_690 (Activation)     (None, 8, 8, 128)    0           add_272[0][0]                    \n__________________________________________________________________________________________________\nglobal_average_pooling2d_23 (Gl (None, 64)           0           activation_675[0][0]             \n__________________________________________________________________________________________________\nglobal_average_pooling2d_24 (Gl (None, 128)          0           activation_690[0][0]             \n__________________________________________________________________________________________________\ndropout_203 (Dropout)           (None, 64)           0           global_average_pooling2d_23[0][0]\n__________________________________________________________________________________________________\ndropout_211 (Dropout)           (None, 128)          0           global_average_pooling2d_24[0][0]\n__________________________________________________________________________________________________\ndense_103 (Dense)               (None, 200)          13000       dropout_203[0][0]                \n__________________________________________________________________________________________________\ndense_106 (Dense)               (None, 1000)         129000      dropout_211[0][0]                \n__________________________________________________________________________________________________\nactivation_676 (Activation)     (None, 200)          0           dense_103[0][0]                  \n__________________________________________________________________________________________________\nactivation_691 (Activation)     (None, 1000)         0           dense_106[0][0]                  \n__________________________________________________________________________________________________\ndropout_204 (Dropout)           (None, 200)          0           activation_676[0][0]             \n__________________________________________________________________________________________________\ndropout_212 (Dropout)           (None, 1000)         0           activation_691[0][0]             \n__________________________________________________________________________________________________\ndense_104 (Dense)               (None, 200)          40200       dropout_204[0][0]                \n__________________________________________________________________________________________________\ndense_107 (Dense)               (None, 1000)         1001000     dropout_212[0][0]                \n__________________________________________________________________________________________________\nactivation_677 (Activation)     (None, 200)          0           dense_104[0][0]                  \n__________________________________________________________________________________________________\nactivation_692 (Activation)     (None, 1000)         0           dense_107[0][0]                  \n__________________________________________________________________________________________________\ndense_105 (Dense)               (None, 10)           2010        activation_677[0][0]             \n__________________________________________________________________________________________________\ndense_108 (Dense)               (None, 10)           10010       activation_692[0][0]             \n==================================================================================================\nTotal params: 2,344,084\nTrainable params: 2,342,228\nNon-trainable params: 1,856\n__________________________________________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from keras.layers import GlobalAveragePooling2D ,GlobalMaxPooling2D, ZeroPadding2D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Input, Add\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "def resnet_identity_block(x,filters,kernel_size,triger = 0,triger_d=0,rate=0.2):\n",
    "    x_skip = x\n",
    "    if triger_d == 1:\n",
    "        x = Dropout(rate)(x)\n",
    "    x = Conv2D(filters=filters,kernel_size=(kernel_size,kernel_size),padding='same')(x)\n",
    "    if triger == 1:\n",
    "        x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Add()([x,x_skip])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def resnet_cov_block(x,filters,kernel_size,triger = 0,triger_d=0,rate=0.2):\n",
    "    x_skip = Conv2D(filters=filters,kernel_size=(kernel_size,kernel_size),padding='same')(x)\n",
    "    if triger == 1:\n",
    "        x_skip = BatchNormalization(axis=3)(x_skip)\n",
    "    x_skip = Activation('relu')(x_skip)\n",
    "    if triger_d == 1:\n",
    "        x = Dropout(rate)(x)\n",
    "    x = Conv2D(filters=filters,kernel_size=(kernel_size,kernel_size),padding='same')(x)\n",
    "    if triger == 1:\n",
    "        x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Add()([x,x_skip])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def auxiliary_output(x):\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(200)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(200)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    return x\n",
    "\n",
    "def resnet(shape = (32,32,3)):\n",
    "    x_input = Input(shape)\n",
    "    x = x_input\n",
    "    x = Conv2D(32,(3,3),padding='same',input_shape=shape, name=\"input_layor\")(x)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = resnet_identity_block(x,32,3,0)\n",
    "    x = resnet_identity_block(x,32,3,0)\n",
    "    x = resnet_identity_block(x,32,3,0)\n",
    "    x = resnet_identity_block(x,32,3,0)\n",
    "    x = MaxPooling2D(2,2)(x)\n",
    "    x = resnet_cov_block(x,64,3,0,1,0.5)\n",
    "    x = resnet_identity_block(x,64,3,0,1,0.5)\n",
    "    x = resnet_identity_block(x,64,3,0,1,0.5)\n",
    "    x = resnet_identity_block(x,64,3,0,1,0.5)\n",
    "    x = resnet_identity_block(x,64,3,0,1,0.5)\n",
    "    x = resnet_identity_block(x,64,3,0,1,0.5)\n",
    "    aux_output_0 = auxiliary_output(x)\n",
    "    x = MaxPooling2D(2,2)(x)\n",
    "    x = resnet_cov_block(x,128,3,1,1,0.5)\n",
    "    x = resnet_identity_block(x,128,3,1,1,0.5)\n",
    "    x = resnet_identity_block(x,128,3,1,1,0.5)\n",
    "    x = resnet_identity_block(x,128,3,1,1,0.5)\n",
    "    x = resnet_identity_block(x,128,3,1,1,0.5)\n",
    "    x = resnet_identity_block(x,128,3,1,1,0.5)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1000)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1000)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    final = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs = x_input, outputs = (aux_output_0,final))\n",
    "    return model\n",
    "\n",
    "model = resnet((32,32,3))\n",
    "model.summary()\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), padding='same',input_shape=(32, 32, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(32, (3, 3), padding= 'same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(32, (3, 3), padding= 'same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), padding= 'same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(64, (3, 3), padding= 'same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Conv2D(128, (3, 3), padding= 'same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(128, (3, 3), padding= 'same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(GlobalMaxPooling2D())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1000))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(500))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "# \n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 3E-4 # to be tuned!\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy','categorical_crossentropy'],\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def batch_generator(x,y,batch_size):\n",
    "    step = 0\n",
    "    while True:\n",
    "        if step == x.shape[0]:\n",
    "            step = 0\n",
    "        x_temp = numpy.zeros(shape=(batch_size,x.shape[1],x.shape[2],x.shape[3]))\n",
    "        y1_temp = numpy.zeros(shape=(batch_size,y.shape[1]))\n",
    "        y2_temp = numpy.zeros(shape=(batch_size,y.shape[1]))\n",
    "        for i in range(batch_size):\n",
    "            x_temp[i] = x[step]\n",
    "            y1_temp[i] = y[step]\n",
    "            y2_temp[i] = y[step]\n",
    "            step+=1\n",
    "        yield(x_temp,[y1_temp,y2_temp])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      " - 15s - loss: 5.2792 - dense_105_loss: 2.9073 - dense_108_loss: 2.3719 - dense_105_acc: 0.1304 - dense_108_acc: 0.1701 - val_loss: 5.5248 - val_dense_105_loss: 2.2253 - val_dense_108_loss: 3.2996 - val_dense_105_acc: 0.1711 - val_dense_108_acc: 0.1008\n",
      "Epoch 2/200\n",
      " - 12s - loss: 3.9483 - dense_105_loss: 2.0488 - dense_108_loss: 1.8995 - dense_105_acc: 0.2124 - dense_108_acc: 0.2535 - val_loss: 5.4108 - val_dense_105_loss: 2.0763 - val_dense_108_loss: 3.3345 - val_dense_105_acc: 0.2269 - val_dense_108_acc: 0.1089\n",
      "Epoch 3/200\n",
      " - 12s - loss: 3.6681 - dense_105_loss: 1.9225 - dense_108_loss: 1.7457 - dense_105_acc: 0.2547 - dense_108_acc: 0.3038 - val_loss: 4.2993 - val_dense_105_loss: 1.9005 - val_dense_108_loss: 2.3987 - val_dense_105_acc: 0.3387 - val_dense_108_acc: 0.1873\n",
      "Epoch 4/200\n",
      " - 12s - loss: 3.4821 - dense_105_loss: 1.8281 - dense_108_loss: 1.6540 - dense_105_acc: 0.2914 - dense_108_acc: 0.3445 - val_loss: 3.7519 - val_dense_105_loss: 1.7313 - val_dense_108_loss: 2.0206 - val_dense_105_acc: 0.3839 - val_dense_108_acc: 0.2505\n",
      "Epoch 5/200\n",
      " - 13s - loss: 3.3458 - dense_105_loss: 1.7593 - dense_108_loss: 1.5865 - dense_105_acc: 0.3189 - dense_108_acc: 0.3785 - val_loss: 3.5583 - val_dense_105_loss: 1.7012 - val_dense_108_loss: 1.8570 - val_dense_105_acc: 0.3586 - val_dense_108_acc: 0.2660\n",
      "Epoch 6/200\n",
      " - 13s - loss: 3.2303 - dense_105_loss: 1.7049 - dense_108_loss: 1.5253 - dense_105_acc: 0.3467 - dense_108_acc: 0.4031 - val_loss: 3.4931 - val_dense_105_loss: 1.6888 - val_dense_108_loss: 1.8044 - val_dense_105_acc: 0.3668 - val_dense_108_acc: 0.2759\n",
      "Epoch 7/200\n",
      " - 13s - loss: 3.1386 - dense_105_loss: 1.6643 - dense_108_loss: 1.4743 - dense_105_acc: 0.3596 - dense_108_acc: 0.4338 - val_loss: 3.0776 - val_dense_105_loss: 1.5169 - val_dense_108_loss: 1.5608 - val_dense_105_acc: 0.4459 - val_dense_108_acc: 0.4017\n",
      "Epoch 8/200\n",
      " - 13s - loss: 3.0495 - dense_105_loss: 1.6285 - dense_108_loss: 1.4210 - dense_105_acc: 0.3808 - dense_108_acc: 0.4575 - val_loss: 3.0251 - val_dense_105_loss: 1.4996 - val_dense_108_loss: 1.5255 - val_dense_105_acc: 0.4498 - val_dense_108_acc: 0.4223\n",
      "Epoch 9/200\n",
      " - 13s - loss: 2.9536 - dense_105_loss: 1.5889 - dense_108_loss: 1.3647 - dense_105_acc: 0.3972 - dense_108_acc: 0.4818 - val_loss: 3.0420 - val_dense_105_loss: 1.5081 - val_dense_108_loss: 1.5339 - val_dense_105_acc: 0.4454 - val_dense_108_acc: 0.4108\n",
      "Epoch 10/200\n",
      " - 13s - loss: 2.8788 - dense_105_loss: 1.5543 - dense_108_loss: 1.3245 - dense_105_acc: 0.4181 - dense_108_acc: 0.5046 - val_loss: 2.7852 - val_dense_105_loss: 1.3825 - val_dense_108_loss: 1.4027 - val_dense_105_acc: 0.5005 - val_dense_108_acc: 0.4897\n",
      "Epoch 11/200\n",
      " - 13s - loss: 2.7912 - dense_105_loss: 1.5123 - dense_108_loss: 1.2789 - dense_105_acc: 0.4319 - dense_108_acc: 0.5257 - val_loss: 2.7827 - val_dense_105_loss: 1.4038 - val_dense_108_loss: 1.3789 - val_dense_105_acc: 0.4866 - val_dense_108_acc: 0.4892\n",
      "Epoch 12/200\n",
      " - 13s - loss: 2.7350 - dense_105_loss: 1.4843 - dense_108_loss: 1.2507 - dense_105_acc: 0.4481 - dense_108_acc: 0.5378 - val_loss: 2.7879 - val_dense_105_loss: 1.3735 - val_dense_108_loss: 1.4144 - val_dense_105_acc: 0.5079 - val_dense_108_acc: 0.4867\n",
      "Epoch 13/200\n",
      " - 13s - loss: 2.6561 - dense_105_loss: 1.4419 - dense_108_loss: 1.2142 - dense_105_acc: 0.4629 - dense_108_acc: 0.5519 - val_loss: 2.6471 - val_dense_105_loss: 1.2893 - val_dense_108_loss: 1.3578 - val_dense_105_acc: 0.5426 - val_dense_108_acc: 0.5379\n",
      "Epoch 14/200\n",
      " - 13s - loss: 2.6034 - dense_105_loss: 1.4199 - dense_108_loss: 1.1835 - dense_105_acc: 0.4732 - dense_108_acc: 0.5664 - val_loss: 2.5842 - val_dense_105_loss: 1.2776 - val_dense_108_loss: 1.3066 - val_dense_105_acc: 0.5411 - val_dense_108_acc: 0.5463\n",
      "Epoch 15/200\n",
      " - 13s - loss: 2.5468 - dense_105_loss: 1.3865 - dense_108_loss: 1.1603 - dense_105_acc: 0.4862 - dense_108_acc: 0.5741 - val_loss: 2.4599 - val_dense_105_loss: 1.2455 - val_dense_108_loss: 1.2144 - val_dense_105_acc: 0.5387 - val_dense_108_acc: 0.5737\n",
      "Epoch 16/200\n",
      " - 13s - loss: 2.4989 - dense_105_loss: 1.3681 - dense_108_loss: 1.1308 - dense_105_acc: 0.4988 - dense_108_acc: 0.5856 - val_loss: 2.5641 - val_dense_105_loss: 1.3069 - val_dense_108_loss: 1.2572 - val_dense_105_acc: 0.5234 - val_dense_108_acc: 0.5731\n",
      "Epoch 17/200\n",
      " - 13s - loss: 2.4582 - dense_105_loss: 1.3469 - dense_108_loss: 1.1113 - dense_105_acc: 0.5031 - dense_108_acc: 0.5964 - val_loss: 2.3975 - val_dense_105_loss: 1.1870 - val_dense_108_loss: 1.2105 - val_dense_105_acc: 0.5700 - val_dense_108_acc: 0.5949\n",
      "Epoch 18/200\n",
      " - 13s - loss: 2.4180 - dense_105_loss: 1.3272 - dense_108_loss: 1.0908 - dense_105_acc: 0.5112 - dense_108_acc: 0.6036 - val_loss: 2.3549 - val_dense_105_loss: 1.1698 - val_dense_108_loss: 1.1851 - val_dense_105_acc: 0.5778 - val_dense_108_acc: 0.6143\n",
      "Epoch 19/200\n",
      " - 13s - loss: 2.3776 - dense_105_loss: 1.3102 - dense_108_loss: 1.0675 - dense_105_acc: 0.5182 - dense_108_acc: 0.6107 - val_loss: 2.3693 - val_dense_105_loss: 1.1651 - val_dense_108_loss: 1.2041 - val_dense_105_acc: 0.5879 - val_dense_108_acc: 0.6180\n",
      "Epoch 20/200\n",
      " - 13s - loss: 2.3293 - dense_105_loss: 1.2808 - dense_108_loss: 1.0486 - dense_105_acc: 0.5296 - dense_108_acc: 0.6205 - val_loss: 2.3318 - val_dense_105_loss: 1.1402 - val_dense_108_loss: 1.1916 - val_dense_105_acc: 0.5885 - val_dense_108_acc: 0.6177\n",
      "Epoch 21/200\n",
      " - 13s - loss: 2.3116 - dense_105_loss: 1.2773 - dense_108_loss: 1.0343 - dense_105_acc: 0.5325 - dense_108_acc: 0.6263 - val_loss: 2.3486 - val_dense_105_loss: 1.1521 - val_dense_108_loss: 1.1965 - val_dense_105_acc: 0.5836 - val_dense_108_acc: 0.6177\n",
      "Epoch 22/200\n",
      " - 13s - loss: 2.2583 - dense_105_loss: 1.2433 - dense_108_loss: 1.0150 - dense_105_acc: 0.5451 - dense_108_acc: 0.6346 - val_loss: 2.2656 - val_dense_105_loss: 1.1037 - val_dense_108_loss: 1.1619 - val_dense_105_acc: 0.6003 - val_dense_108_acc: 0.6291\n",
      "Epoch 23/200\n",
      " - 13s - loss: 2.2320 - dense_105_loss: 1.2352 - dense_108_loss: 0.9968 - dense_105_acc: 0.5490 - dense_108_acc: 0.6389 - val_loss: 2.2539 - val_dense_105_loss: 1.1078 - val_dense_108_loss: 1.1460 - val_dense_105_acc: 0.6069 - val_dense_108_acc: 0.6381\n",
      "Epoch 24/200\n",
      " - 13s - loss: 2.2038 - dense_105_loss: 1.2217 - dense_108_loss: 0.9821 - dense_105_acc: 0.5557 - dense_108_acc: 0.6449 - val_loss: 2.2637 - val_dense_105_loss: 1.1228 - val_dense_108_loss: 1.1409 - val_dense_105_acc: 0.6023 - val_dense_108_acc: 0.6345\n",
      "Epoch 25/200\n",
      " - 13s - loss: 2.1668 - dense_105_loss: 1.2006 - dense_108_loss: 0.9662 - dense_105_acc: 0.5633 - dense_108_acc: 0.6528 - val_loss: 2.1750 - val_dense_105_loss: 1.0681 - val_dense_108_loss: 1.1069 - val_dense_105_acc: 0.6162 - val_dense_108_acc: 0.6503\n",
      "Epoch 26/200\n",
      " - 13s - loss: 2.1379 - dense_105_loss: 1.1844 - dense_108_loss: 0.9535 - dense_105_acc: 0.5665 - dense_108_acc: 0.6586 - val_loss: 2.1354 - val_dense_105_loss: 1.0447 - val_dense_108_loss: 1.0907 - val_dense_105_acc: 0.6254 - val_dense_108_acc: 0.6479\n",
      "Epoch 27/200\n",
      " - 13s - loss: 2.1159 - dense_105_loss: 1.1779 - dense_108_loss: 0.9379 - dense_105_acc: 0.5737 - dense_108_acc: 0.6624 - val_loss: 2.1747 - val_dense_105_loss: 1.0522 - val_dense_108_loss: 1.1225 - val_dense_105_acc: 0.6264 - val_dense_108_acc: 0.6368\n",
      "Epoch 28/200\n",
      " - 13s - loss: 2.0828 - dense_105_loss: 1.1571 - dense_108_loss: 0.9257 - dense_105_acc: 0.5794 - dense_108_acc: 0.6676 - val_loss: 2.1689 - val_dense_105_loss: 1.0721 - val_dense_108_loss: 1.0968 - val_dense_105_acc: 0.6123 - val_dense_108_acc: 0.6477\n",
      "Epoch 29/200\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# from keras.callbacks.callbacks import EarlyStopping\n",
    "# callback = EarlyStopping(monitor='val_acc',mode='max', verbose=1, min_delta=1E-8, patience=10, restore_best_weights=True)\n",
    "\n",
    "batch_size = 400\n",
    "history = model.fit_generator(batch_generator(x_tr,y_tr,batch_size=batch_size), steps_per_epoch=x_tr.shape[0]//batch_size,epochs=200, verbose=2, validation_data=(x_val, [y_val,y_val]))\n",
    "# history = model.fit(x_tr, [y_tr,y_tr], batch_size=batch_size, epochs= 300, verbose=2, validation_data=(x_val, [y_val,y_val]))\n",
    "# history = model.fit_generator(datagen.flow(x_tr, y_tr, batch_size=batch_size), steps_per_epoch=x_tr.shape[0]//batch_size,epochs=200, verbose=2, validation_data=(x_val, y_val))\n",
    "# history = model.fit(x_tr, y_tr, batch_size=batch_size, epochs=50, verbose=2, validation_data=(x_val, y_val))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-577833b341d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dense_84_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_dense_84_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dense_84_acc'"
     ],
     "ename": "KeyError",
     "evalue": "'dense_84_acc'",
     "output_type": "error"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['dense_84_acc']\n",
    "val_acc = history.history['val_dense_84_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 3E-4 # to be tuned!\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy','categorical_crossentropy'],\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      " - 19s - loss: 4.1086 - dense_87_loss: 2.0673 - dense_90_loss: 2.0412 - dense_87_acc: 0.2172 - dense_90_acc: 0.2284\n",
      "Epoch 2/200\n",
      " - 17s - loss: 3.4158 - dense_87_loss: 1.7408 - dense_90_loss: 1.6750 - dense_87_acc: 0.3329 - dense_90_acc: 0.3634\n",
      "Epoch 3/200\n",
      " - 17s - loss: 3.1232 - dense_87_loss: 1.6113 - dense_90_loss: 1.5119 - dense_87_acc: 0.3890 - dense_90_acc: 0.4339\n",
      "Epoch 4/200\n",
      " - 17s - loss: 2.9093 - dense_87_loss: 1.5147 - dense_90_loss: 1.3946 - dense_87_acc: 0.4286 - dense_90_acc: 0.4855\n",
      "Epoch 5/200\n",
      " - 17s - loss: 2.7255 - dense_87_loss: 1.4278 - dense_90_loss: 1.2977 - dense_87_acc: 0.4698 - dense_90_acc: 0.5254\n",
      "Epoch 6/200\n",
      " - 17s - loss: 2.5711 - dense_87_loss: 1.3579 - dense_90_loss: 1.2131 - dense_87_acc: 0.4973 - dense_90_acc: 0.5619\n",
      "Epoch 7/200\n",
      " - 17s - loss: 2.4456 - dense_87_loss: 1.2950 - dense_90_loss: 1.1506 - dense_87_acc: 0.5241 - dense_90_acc: 0.5838\n",
      "Epoch 8/200\n",
      " - 17s - loss: 2.3419 - dense_87_loss: 1.2504 - dense_90_loss: 1.0915 - dense_87_acc: 0.5422 - dense_90_acc: 0.6067\n",
      "Epoch 9/200\n",
      " - 17s - loss: 2.2487 - dense_87_loss: 1.2059 - dense_90_loss: 1.0428 - dense_87_acc: 0.5596 - dense_90_acc: 0.6264\n",
      "Epoch 10/200\n",
      " - 17s - loss: 2.1664 - dense_87_loss: 1.1660 - dense_90_loss: 1.0004 - dense_87_acc: 0.5748 - dense_90_acc: 0.6419\n",
      "Epoch 11/200\n",
      " - 17s - loss: 2.0924 - dense_87_loss: 1.1332 - dense_90_loss: 0.9592 - dense_87_acc: 0.5893 - dense_90_acc: 0.6603\n",
      "Epoch 12/200\n",
      " - 17s - loss: 2.0209 - dense_87_loss: 1.1001 - dense_90_loss: 0.9208 - dense_87_acc: 0.6036 - dense_90_acc: 0.6730\n",
      "Epoch 13/200\n",
      " - 17s - loss: 1.9617 - dense_87_loss: 1.0693 - dense_90_loss: 0.8924 - dense_87_acc: 0.6147 - dense_90_acc: 0.6834\n",
      "Epoch 14/200\n",
      " - 17s - loss: 1.8985 - dense_87_loss: 1.0395 - dense_90_loss: 0.8589 - dense_87_acc: 0.6277 - dense_90_acc: 0.6984\n",
      "Epoch 15/200\n",
      " - 17s - loss: 1.8466 - dense_87_loss: 1.0168 - dense_90_loss: 0.8299 - dense_87_acc: 0.6356 - dense_90_acc: 0.7071\n",
      "Epoch 16/200\n",
      " - 17s - loss: 1.7952 - dense_87_loss: 0.9896 - dense_90_loss: 0.8055 - dense_87_acc: 0.6444 - dense_90_acc: 0.7167\n",
      "Epoch 17/200\n",
      " - 17s - loss: 1.7531 - dense_87_loss: 0.9715 - dense_90_loss: 0.7816 - dense_87_acc: 0.6545 - dense_90_acc: 0.7254\n",
      "Epoch 18/200\n",
      " - 17s - loss: 1.7123 - dense_87_loss: 0.9563 - dense_90_loss: 0.7560 - dense_87_acc: 0.6585 - dense_90_acc: 0.7359\n",
      "Epoch 19/200\n",
      " - 17s - loss: 1.6658 - dense_87_loss: 0.9337 - dense_90_loss: 0.7320 - dense_87_acc: 0.6664 - dense_90_acc: 0.7423\n",
      "Epoch 20/200\n",
      " - 17s - loss: 1.6266 - dense_87_loss: 0.9186 - dense_90_loss: 0.7079 - dense_87_acc: 0.6722 - dense_90_acc: 0.7535\n",
      "Epoch 21/200\n",
      " - 17s - loss: 1.5871 - dense_87_loss: 0.8979 - dense_90_loss: 0.6892 - dense_87_acc: 0.6826 - dense_90_acc: 0.7591\n",
      "Epoch 22/200\n",
      " - 17s - loss: 1.5598 - dense_87_loss: 0.8896 - dense_90_loss: 0.6702 - dense_87_acc: 0.6853 - dense_90_acc: 0.7641\n",
      "Epoch 23/200\n",
      " - 17s - loss: 1.5240 - dense_87_loss: 0.8688 - dense_90_loss: 0.6552 - dense_87_acc: 0.6923 - dense_90_acc: 0.7717\n",
      "Epoch 24/200\n",
      " - 17s - loss: 1.4985 - dense_87_loss: 0.8569 - dense_90_loss: 0.6416 - dense_87_acc: 0.6978 - dense_90_acc: 0.7754\n",
      "Epoch 25/200\n",
      " - 17s - loss: 1.4641 - dense_87_loss: 0.8412 - dense_90_loss: 0.6229 - dense_87_acc: 0.7024 - dense_90_acc: 0.7838\n",
      "Epoch 26/200\n",
      " - 17s - loss: 1.4492 - dense_87_loss: 0.8368 - dense_90_loss: 0.6123 - dense_87_acc: 0.7035 - dense_90_acc: 0.7877\n",
      "Epoch 27/200\n",
      " - 17s - loss: 1.4194 - dense_87_loss: 0.8228 - dense_90_loss: 0.5967 - dense_87_acc: 0.7092 - dense_90_acc: 0.7927\n",
      "Epoch 28/200\n",
      " - 17s - loss: 1.3903 - dense_87_loss: 0.8077 - dense_90_loss: 0.5827 - dense_87_acc: 0.7158 - dense_90_acc: 0.7970\n",
      "Epoch 29/200\n",
      " - 17s - loss: 1.3645 - dense_87_loss: 0.7994 - dense_90_loss: 0.5651 - dense_87_acc: 0.7179 - dense_90_acc: 0.8035\n",
      "Epoch 30/200\n",
      " - 17s - loss: 1.3467 - dense_87_loss: 0.7886 - dense_90_loss: 0.5581 - dense_87_acc: 0.7211 - dense_90_acc: 0.8051\n",
      "Epoch 31/200\n",
      " - 17s - loss: 1.3236 - dense_87_loss: 0.7799 - dense_90_loss: 0.5437 - dense_87_acc: 0.7263 - dense_90_acc: 0.8104\n",
      "Epoch 32/200\n",
      " - 17s - loss: 1.3029 - dense_87_loss: 0.7681 - dense_90_loss: 0.5348 - dense_87_acc: 0.7288 - dense_90_acc: 0.8152\n",
      "Epoch 33/200\n",
      " - 17s - loss: 1.2820 - dense_87_loss: 0.7611 - dense_90_loss: 0.5209 - dense_87_acc: 0.7335 - dense_90_acc: 0.8190\n",
      "Epoch 34/200\n",
      " - 17s - loss: 1.2602 - dense_87_loss: 0.7513 - dense_90_loss: 0.5088 - dense_87_acc: 0.7354 - dense_90_acc: 0.8243\n",
      "Epoch 35/200\n",
      " - 17s - loss: 1.2424 - dense_87_loss: 0.7416 - dense_90_loss: 0.5008 - dense_87_acc: 0.7386 - dense_90_acc: 0.8255\n",
      "Epoch 36/200\n",
      " - 17s - loss: 1.2229 - dense_87_loss: 0.7330 - dense_90_loss: 0.4899 - dense_87_acc: 0.7412 - dense_90_acc: 0.8315\n",
      "Epoch 37/200\n",
      " - 17s - loss: 1.2118 - dense_87_loss: 0.7276 - dense_90_loss: 0.4842 - dense_87_acc: 0.7455 - dense_90_acc: 0.8331\n",
      "Epoch 38/200\n",
      " - 17s - loss: 1.1946 - dense_87_loss: 0.7216 - dense_90_loss: 0.4730 - dense_87_acc: 0.7453 - dense_90_acc: 0.8344\n",
      "Epoch 39/200\n",
      " - 17s - loss: 1.1720 - dense_87_loss: 0.7103 - dense_90_loss: 0.4617 - dense_87_acc: 0.7502 - dense_90_acc: 0.8415\n",
      "Epoch 40/200\n",
      " - 17s - loss: 1.1586 - dense_87_loss: 0.7020 - dense_90_loss: 0.4567 - dense_87_acc: 0.7535 - dense_90_acc: 0.8424\n",
      "Epoch 41/200\n",
      " - 17s - loss: 1.1464 - dense_87_loss: 0.6965 - dense_90_loss: 0.4499 - dense_87_acc: 0.7559 - dense_90_acc: 0.8446\n",
      "Epoch 42/200\n",
      " - 17s - loss: 1.1297 - dense_87_loss: 0.6903 - dense_90_loss: 0.4394 - dense_87_acc: 0.7599 - dense_90_acc: 0.8485\n",
      "Epoch 43/200\n",
      " - 17s - loss: 1.1137 - dense_87_loss: 0.6844 - dense_90_loss: 0.4293 - dense_87_acc: 0.7604 - dense_90_acc: 0.8517\n",
      "Epoch 44/200\n",
      " - 17s - loss: 1.0912 - dense_87_loss: 0.6725 - dense_90_loss: 0.4187 - dense_87_acc: 0.7661 - dense_90_acc: 0.8544\n",
      "Epoch 45/200\n",
      " - 17s - loss: 1.0845 - dense_87_loss: 0.6656 - dense_90_loss: 0.4189 - dense_87_acc: 0.7663 - dense_90_acc: 0.8549\n",
      "Epoch 46/200\n",
      " - 17s - loss: 1.0747 - dense_87_loss: 0.6628 - dense_90_loss: 0.4119 - dense_87_acc: 0.7672 - dense_90_acc: 0.8574\n",
      "Epoch 47/200\n",
      " - 17s - loss: 1.0570 - dense_87_loss: 0.6579 - dense_90_loss: 0.3990 - dense_87_acc: 0.7716 - dense_90_acc: 0.8603\n",
      "Epoch 48/200\n",
      " - 17s - loss: 1.0405 - dense_87_loss: 0.6498 - dense_90_loss: 0.3907 - dense_87_acc: 0.7709 - dense_90_acc: 0.8641\n",
      "Epoch 49/200\n",
      " - 17s - loss: 1.0298 - dense_87_loss: 0.6460 - dense_90_loss: 0.3838 - dense_87_acc: 0.7743 - dense_90_acc: 0.8657\n",
      "Epoch 50/200\n",
      " - 17s - loss: 1.0161 - dense_87_loss: 0.6405 - dense_90_loss: 0.3757 - dense_87_acc: 0.7744 - dense_90_acc: 0.8712\n",
      "Epoch 51/200\n",
      " - 17s - loss: 0.9989 - dense_87_loss: 0.6284 - dense_90_loss: 0.3705 - dense_87_acc: 0.7816 - dense_90_acc: 0.8702\n",
      "Epoch 52/200\n",
      " - 17s - loss: 1.0025 - dense_87_loss: 0.6311 - dense_90_loss: 0.3714 - dense_87_acc: 0.7785 - dense_90_acc: 0.8707\n",
      "Epoch 53/200\n",
      " - 17s - loss: 0.9850 - dense_87_loss: 0.6257 - dense_90_loss: 0.3592 - dense_87_acc: 0.7820 - dense_90_acc: 0.8741\n",
      "Epoch 54/200\n",
      " - 17s - loss: 0.9699 - dense_87_loss: 0.6177 - dense_90_loss: 0.3522 - dense_87_acc: 0.7835 - dense_90_acc: 0.8766\n",
      "Epoch 55/200\n",
      " - 17s - loss: 0.9542 - dense_87_loss: 0.6091 - dense_90_loss: 0.3451 - dense_87_acc: 0.7870 - dense_90_acc: 0.8784\n",
      "Epoch 56/200\n",
      " - 17s - loss: 0.9466 - dense_87_loss: 0.6055 - dense_90_loss: 0.3410 - dense_87_acc: 0.7890 - dense_90_acc: 0.8791\n",
      "Epoch 57/200\n",
      " - 17s - loss: 0.9357 - dense_87_loss: 0.6031 - dense_90_loss: 0.3327 - dense_87_acc: 0.7907 - dense_90_acc: 0.8838\n",
      "Epoch 58/200\n",
      " - 17s - loss: 0.9290 - dense_87_loss: 0.6005 - dense_90_loss: 0.3285 - dense_87_acc: 0.7910 - dense_90_acc: 0.8862\n",
      "Epoch 59/200\n",
      " - 17s - loss: 0.9195 - dense_87_loss: 0.5966 - dense_90_loss: 0.3230 - dense_87_acc: 0.7912 - dense_90_acc: 0.8873\n",
      "Epoch 60/200\n",
      " - 17s - loss: 0.9114 - dense_87_loss: 0.5914 - dense_90_loss: 0.3201 - dense_87_acc: 0.7926 - dense_90_acc: 0.8888\n",
      "Epoch 61/200\n",
      " - 17s - loss: 0.9025 - dense_87_loss: 0.5891 - dense_90_loss: 0.3134 - dense_87_acc: 0.7938 - dense_90_acc: 0.8901\n",
      "Epoch 62/200\n",
      " - 17s - loss: 0.8906 - dense_87_loss: 0.5831 - dense_90_loss: 0.3075 - dense_87_acc: 0.7962 - dense_90_acc: 0.8914\n",
      "Epoch 63/200\n",
      " - 17s - loss: 0.8858 - dense_87_loss: 0.5821 - dense_90_loss: 0.3037 - dense_87_acc: 0.7985 - dense_90_acc: 0.8934\n",
      "Epoch 64/200\n",
      " - 17s - loss: 0.8703 - dense_87_loss: 0.5789 - dense_90_loss: 0.2914 - dense_87_acc: 0.7977 - dense_90_acc: 0.8968\n",
      "Epoch 65/200\n",
      " - 17s - loss: 0.8657 - dense_87_loss: 0.5697 - dense_90_loss: 0.2960 - dense_87_acc: 0.8011 - dense_90_acc: 0.8956\n",
      "Epoch 66/200\n",
      " - 17s - loss: 0.8605 - dense_87_loss: 0.5713 - dense_90_loss: 0.2892 - dense_87_acc: 0.8013 - dense_90_acc: 0.8991\n",
      "Epoch 67/200\n",
      " - 17s - loss: 0.8496 - dense_87_loss: 0.5649 - dense_90_loss: 0.2847 - dense_87_acc: 0.8029 - dense_90_acc: 0.8994\n",
      "Epoch 68/200\n",
      " - 17s - loss: 0.8408 - dense_87_loss: 0.5605 - dense_90_loss: 0.2803 - dense_87_acc: 0.8047 - dense_90_acc: 0.9015\n",
      "Epoch 69/200\n",
      " - 17s - loss: 0.8262 - dense_87_loss: 0.5547 - dense_90_loss: 0.2715 - dense_87_acc: 0.8074 - dense_90_acc: 0.9051\n",
      "Epoch 70/200\n",
      " - 17s - loss: 0.8202 - dense_87_loss: 0.5535 - dense_90_loss: 0.2667 - dense_87_acc: 0.8070 - dense_90_acc: 0.9061\n",
      "Epoch 71/200\n",
      " - 17s - loss: 0.8140 - dense_87_loss: 0.5489 - dense_90_loss: 0.2651 - dense_87_acc: 0.8080 - dense_90_acc: 0.9067\n",
      "Epoch 72/200\n",
      " - 17s - loss: 0.8032 - dense_87_loss: 0.5474 - dense_90_loss: 0.2558 - dense_87_acc: 0.8101 - dense_90_acc: 0.9089\n",
      "Epoch 73/200\n",
      " - 17s - loss: 0.7974 - dense_87_loss: 0.5409 - dense_90_loss: 0.2565 - dense_87_acc: 0.8126 - dense_90_acc: 0.9094\n",
      "Epoch 74/200\n",
      " - 17s - loss: 0.7839 - dense_87_loss: 0.5366 - dense_90_loss: 0.2473 - dense_87_acc: 0.8143 - dense_90_acc: 0.9128\n",
      "Epoch 75/200\n",
      " - 17s - loss: 0.7833 - dense_87_loss: 0.5326 - dense_90_loss: 0.2507 - dense_87_acc: 0.8148 - dense_90_acc: 0.9126\n",
      "Epoch 76/200\n",
      " - 17s - loss: 0.7808 - dense_87_loss: 0.5312 - dense_90_loss: 0.2496 - dense_87_acc: 0.8150 - dense_90_acc: 0.9122\n",
      "Epoch 77/200\n",
      " - 17s - loss: 0.7671 - dense_87_loss: 0.5249 - dense_90_loss: 0.2423 - dense_87_acc: 0.8184 - dense_90_acc: 0.9144\n",
      "Epoch 78/200\n",
      " - 17s - loss: 0.7609 - dense_87_loss: 0.5264 - dense_90_loss: 0.2345 - dense_87_acc: 0.8166 - dense_90_acc: 0.9166\n",
      "Epoch 79/200\n",
      " - 17s - loss: 0.7536 - dense_87_loss: 0.5230 - dense_90_loss: 0.2307 - dense_87_acc: 0.8180 - dense_90_acc: 0.9170\n",
      "Epoch 80/200\n",
      " - 17s - loss: 0.7540 - dense_87_loss: 0.5240 - dense_90_loss: 0.2301 - dense_87_acc: 0.8178 - dense_90_acc: 0.9194\n",
      "Epoch 81/200\n",
      " - 17s - loss: 0.7466 - dense_87_loss: 0.5183 - dense_90_loss: 0.2282 - dense_87_acc: 0.8206 - dense_90_acc: 0.9205\n",
      "Epoch 82/200\n",
      " - 17s - loss: 0.7379 - dense_87_loss: 0.5135 - dense_90_loss: 0.2244 - dense_87_acc: 0.8203 - dense_90_acc: 0.9220\n",
      "Epoch 83/200\n",
      " - 17s - loss: 0.7303 - dense_87_loss: 0.5093 - dense_90_loss: 0.2210 - dense_87_acc: 0.8224 - dense_90_acc: 0.9222\n",
      "Epoch 84/200\n",
      " - 17s - loss: 0.7228 - dense_87_loss: 0.5058 - dense_90_loss: 0.2170 - dense_87_acc: 0.8252 - dense_90_acc: 0.9241\n",
      "Epoch 85/200\n",
      " - 17s - loss: 0.7224 - dense_87_loss: 0.5078 - dense_90_loss: 0.2147 - dense_87_acc: 0.8235 - dense_90_acc: 0.9249\n",
      "Epoch 86/200\n",
      " - 17s - loss: 0.7131 - dense_87_loss: 0.5053 - dense_90_loss: 0.2078 - dense_87_acc: 0.8247 - dense_90_acc: 0.9272\n",
      "Epoch 87/200\n",
      " - 17s - loss: 0.7080 - dense_87_loss: 0.4985 - dense_90_loss: 0.2096 - dense_87_acc: 0.8261 - dense_90_acc: 0.9265\n",
      "Epoch 88/200\n",
      " - 17s - loss: 0.6992 - dense_87_loss: 0.4949 - dense_90_loss: 0.2043 - dense_87_acc: 0.8263 - dense_90_acc: 0.9274\n",
      "Epoch 89/200\n",
      " - 17s - loss: 0.6969 - dense_87_loss: 0.4963 - dense_90_loss: 0.2006 - dense_87_acc: 0.8274 - dense_90_acc: 0.9303\n",
      "Epoch 90/200\n",
      " - 17s - loss: 0.6927 - dense_87_loss: 0.4937 - dense_90_loss: 0.1990 - dense_87_acc: 0.8286 - dense_90_acc: 0.9297\n",
      "Epoch 91/200\n",
      " - 17s - loss: 0.6876 - dense_87_loss: 0.4912 - dense_90_loss: 0.1964 - dense_87_acc: 0.8299 - dense_90_acc: 0.9314\n",
      "Epoch 92/200\n",
      " - 17s - loss: 0.6859 - dense_87_loss: 0.4899 - dense_90_loss: 0.1961 - dense_87_acc: 0.8286 - dense_90_acc: 0.9310\n",
      "Epoch 93/200\n",
      " - 17s - loss: 0.6735 - dense_87_loss: 0.4862 - dense_90_loss: 0.1873 - dense_87_acc: 0.8307 - dense_90_acc: 0.9348\n",
      "Epoch 94/200\n",
      " - 17s - loss: 0.6709 - dense_87_loss: 0.4813 - dense_90_loss: 0.1896 - dense_87_acc: 0.8311 - dense_90_acc: 0.9332\n",
      "Epoch 95/200\n",
      " - 17s - loss: 0.6650 - dense_87_loss: 0.4818 - dense_90_loss: 0.1832 - dense_87_acc: 0.8339 - dense_90_acc: 0.9359\n",
      "Epoch 96/200\n",
      " - 17s - loss: 0.6555 - dense_87_loss: 0.4749 - dense_90_loss: 0.1807 - dense_87_acc: 0.8332 - dense_90_acc: 0.9361\n",
      "Epoch 97/200\n",
      " - 17s - loss: 0.6573 - dense_87_loss: 0.4755 - dense_90_loss: 0.1818 - dense_87_acc: 0.8348 - dense_90_acc: 0.9369\n",
      "Epoch 98/200\n",
      " - 17s - loss: 0.6521 - dense_87_loss: 0.4732 - dense_90_loss: 0.1789 - dense_87_acc: 0.8353 - dense_90_acc: 0.9366\n",
      "Epoch 99/200\n",
      " - 17s - loss: 0.6455 - dense_87_loss: 0.4713 - dense_90_loss: 0.1741 - dense_87_acc: 0.8356 - dense_90_acc: 0.9388\n",
      "Epoch 100/200\n",
      " - 17s - loss: 0.6427 - dense_87_loss: 0.4665 - dense_90_loss: 0.1761 - dense_87_acc: 0.8376 - dense_90_acc: 0.9386\n",
      "Epoch 101/200\n",
      " - 17s - loss: 0.6389 - dense_87_loss: 0.4660 - dense_90_loss: 0.1729 - dense_87_acc: 0.8373 - dense_90_acc: 0.9394\n",
      "Epoch 102/200\n",
      " - 17s - loss: 0.6329 - dense_87_loss: 0.4647 - dense_90_loss: 0.1683 - dense_87_acc: 0.8373 - dense_90_acc: 0.9419\n",
      "Epoch 103/200\n",
      " - 17s - loss: 0.6294 - dense_87_loss: 0.4616 - dense_90_loss: 0.1678 - dense_87_acc: 0.8391 - dense_90_acc: 0.9412\n",
      "Epoch 104/200\n",
      " - 17s - loss: 0.6298 - dense_87_loss: 0.4667 - dense_90_loss: 0.1631 - dense_87_acc: 0.8373 - dense_90_acc: 0.9440\n",
      "Epoch 105/200\n",
      " - 17s - loss: 0.6183 - dense_87_loss: 0.4583 - dense_90_loss: 0.1601 - dense_87_acc: 0.8391 - dense_90_acc: 0.9440\n",
      "Epoch 106/200\n",
      " - 17s - loss: 0.6180 - dense_87_loss: 0.4563 - dense_90_loss: 0.1617 - dense_87_acc: 0.8419 - dense_90_acc: 0.9431\n",
      "Epoch 107/200\n",
      " - 17s - loss: 0.6185 - dense_87_loss: 0.4573 - dense_90_loss: 0.1612 - dense_87_acc: 0.8397 - dense_90_acc: 0.9445\n",
      "Epoch 108/200\n",
      " - 17s - loss: 0.6119 - dense_87_loss: 0.4583 - dense_90_loss: 0.1536 - dense_87_acc: 0.8399 - dense_90_acc: 0.9465\n",
      "Epoch 109/200\n",
      " - 17s - loss: 0.5991 - dense_87_loss: 0.4443 - dense_90_loss: 0.1548 - dense_87_acc: 0.8447 - dense_90_acc: 0.9462\n",
      "Epoch 110/200\n",
      " - 17s - loss: 0.5976 - dense_87_loss: 0.4471 - dense_90_loss: 0.1505 - dense_87_acc: 0.8437 - dense_90_acc: 0.9477\n",
      "Epoch 111/200\n",
      " - 17s - loss: 0.5920 - dense_87_loss: 0.4460 - dense_90_loss: 0.1460 - dense_87_acc: 0.8456 - dense_90_acc: 0.9485\n",
      "Epoch 112/200\n",
      " - 17s - loss: 0.5944 - dense_87_loss: 0.4480 - dense_90_loss: 0.1464 - dense_87_acc: 0.8451 - dense_90_acc: 0.9496\n",
      "Epoch 113/200\n",
      " - 17s - loss: 0.5917 - dense_87_loss: 0.4429 - dense_90_loss: 0.1488 - dense_87_acc: 0.8467 - dense_90_acc: 0.9475\n",
      "Epoch 114/200\n",
      " - 17s - loss: 0.5825 - dense_87_loss: 0.4353 - dense_90_loss: 0.1472 - dense_87_acc: 0.8478 - dense_90_acc: 0.9482\n",
      "Epoch 115/200\n",
      " - 17s - loss: 0.5782 - dense_87_loss: 0.4383 - dense_90_loss: 0.1399 - dense_87_acc: 0.8471 - dense_90_acc: 0.9511\n",
      "Epoch 116/200\n",
      " - 17s - loss: 0.5817 - dense_87_loss: 0.4403 - dense_90_loss: 0.1414 - dense_87_acc: 0.8453 - dense_90_acc: 0.9503\n",
      "Epoch 117/200\n",
      " - 17s - loss: 0.5848 - dense_87_loss: 0.4410 - dense_90_loss: 0.1438 - dense_87_acc: 0.8468 - dense_90_acc: 0.9502\n",
      "Epoch 118/200\n",
      " - 17s - loss: 0.5724 - dense_87_loss: 0.4321 - dense_90_loss: 0.1402 - dense_87_acc: 0.8478 - dense_90_acc: 0.9514\n",
      "Epoch 119/200\n",
      " - 17s - loss: 0.5670 - dense_87_loss: 0.4278 - dense_90_loss: 0.1392 - dense_87_acc: 0.8506 - dense_90_acc: 0.9519\n",
      "Epoch 120/200\n",
      " - 17s - loss: 0.5602 - dense_87_loss: 0.4276 - dense_90_loss: 0.1326 - dense_87_acc: 0.8500 - dense_90_acc: 0.9538\n",
      "Epoch 121/200\n",
      " - 17s - loss: 0.5562 - dense_87_loss: 0.4255 - dense_90_loss: 0.1307 - dense_87_acc: 0.8518 - dense_90_acc: 0.9544\n",
      "Epoch 122/200\n",
      " - 17s - loss: 0.5565 - dense_87_loss: 0.4221 - dense_90_loss: 0.1345 - dense_87_acc: 0.8541 - dense_90_acc: 0.9523\n",
      "Epoch 123/200\n",
      " - 17s - loss: 0.5560 - dense_87_loss: 0.4222 - dense_90_loss: 0.1338 - dense_87_acc: 0.8530 - dense_90_acc: 0.9530\n",
      "Epoch 124/200\n",
      " - 17s - loss: 0.5475 - dense_87_loss: 0.4186 - dense_90_loss: 0.1289 - dense_87_acc: 0.8555 - dense_90_acc: 0.9552\n",
      "Epoch 125/200\n",
      " - 17s - loss: 0.5516 - dense_87_loss: 0.4242 - dense_90_loss: 0.1274 - dense_87_acc: 0.8526 - dense_90_acc: 0.9560\n",
      "Epoch 126/200\n",
      " - 17s - loss: 0.5507 - dense_87_loss: 0.4219 - dense_90_loss: 0.1288 - dense_87_acc: 0.8514 - dense_90_acc: 0.9552\n",
      "Epoch 127/200\n",
      " - 17s - loss: 0.5468 - dense_87_loss: 0.4203 - dense_90_loss: 0.1265 - dense_87_acc: 0.8545 - dense_90_acc: 0.9559\n",
      "Epoch 128/200\n",
      " - 17s - loss: 0.5363 - dense_87_loss: 0.4140 - dense_90_loss: 0.1224 - dense_87_acc: 0.8558 - dense_90_acc: 0.9581\n",
      "Epoch 129/200\n",
      " - 17s - loss: 0.5354 - dense_87_loss: 0.4121 - dense_90_loss: 0.1232 - dense_87_acc: 0.8568 - dense_90_acc: 0.9565\n",
      "Epoch 130/200\n",
      " - 17s - loss: 0.5297 - dense_87_loss: 0.4094 - dense_90_loss: 0.1203 - dense_87_acc: 0.8558 - dense_90_acc: 0.9587\n",
      "Epoch 131/200\n",
      " - 17s - loss: 0.5266 - dense_87_loss: 0.4087 - dense_90_loss: 0.1179 - dense_87_acc: 0.8563 - dense_90_acc: 0.9594\n",
      "Epoch 132/200\n",
      " - 17s - loss: 0.5306 - dense_87_loss: 0.4098 - dense_90_loss: 0.1208 - dense_87_acc: 0.8573 - dense_90_acc: 0.9585\n",
      "Epoch 133/200\n",
      " - 17s - loss: 0.5213 - dense_87_loss: 0.4063 - dense_90_loss: 0.1151 - dense_87_acc: 0.8592 - dense_90_acc: 0.9606\n",
      "Epoch 134/200\n",
      " - 17s - loss: 0.5288 - dense_87_loss: 0.4081 - dense_90_loss: 0.1207 - dense_87_acc: 0.8590 - dense_90_acc: 0.9592\n",
      "Epoch 135/200\n",
      " - 17s - loss: 0.5190 - dense_87_loss: 0.4041 - dense_90_loss: 0.1150 - dense_87_acc: 0.8616 - dense_90_acc: 0.9610\n",
      "Epoch 136/200\n",
      " - 17s - loss: 0.5152 - dense_87_loss: 0.4002 - dense_90_loss: 0.1150 - dense_87_acc: 0.8601 - dense_90_acc: 0.9608\n",
      "Epoch 137/200\n",
      " - 17s - loss: 0.5094 - dense_87_loss: 0.3972 - dense_90_loss: 0.1121 - dense_87_acc: 0.8608 - dense_90_acc: 0.9604\n",
      "Epoch 138/200\n",
      " - 17s - loss: 0.5134 - dense_87_loss: 0.3997 - dense_90_loss: 0.1137 - dense_87_acc: 0.8611 - dense_90_acc: 0.9608\n",
      "Epoch 139/200\n",
      " - 17s - loss: 0.5110 - dense_87_loss: 0.3978 - dense_90_loss: 0.1132 - dense_87_acc: 0.8623 - dense_90_acc: 0.9614\n",
      "Epoch 140/200\n",
      " - 17s - loss: 0.5091 - dense_87_loss: 0.3957 - dense_90_loss: 0.1134 - dense_87_acc: 0.8615 - dense_90_acc: 0.9611\n",
      "Epoch 141/200\n",
      " - 17s - loss: 0.5082 - dense_87_loss: 0.3968 - dense_90_loss: 0.1114 - dense_87_acc: 0.8617 - dense_90_acc: 0.9617\n",
      "Epoch 142/200\n",
      " - 17s - loss: 0.5016 - dense_87_loss: 0.3941 - dense_90_loss: 0.1075 - dense_87_acc: 0.8618 - dense_90_acc: 0.9621\n",
      "Epoch 143/200\n",
      " - 17s - loss: 0.4992 - dense_87_loss: 0.3923 - dense_90_loss: 0.1069 - dense_87_acc: 0.8640 - dense_90_acc: 0.9631\n",
      "Epoch 144/200\n",
      " - 17s - loss: 0.4967 - dense_87_loss: 0.3869 - dense_90_loss: 0.1098 - dense_87_acc: 0.8635 - dense_90_acc: 0.9624\n",
      "Epoch 145/200\n",
      " - 17s - loss: 0.4911 - dense_87_loss: 0.3858 - dense_90_loss: 0.1054 - dense_87_acc: 0.8671 - dense_90_acc: 0.9634\n",
      "Epoch 146/200\n",
      " - 17s - loss: 0.4909 - dense_87_loss: 0.3840 - dense_90_loss: 0.1069 - dense_87_acc: 0.8660 - dense_90_acc: 0.9633\n",
      "Epoch 147/200\n",
      " - 17s - loss: 0.4860 - dense_87_loss: 0.3810 - dense_90_loss: 0.1050 - dense_87_acc: 0.8672 - dense_90_acc: 0.9641\n",
      "Epoch 148/200\n",
      " - 17s - loss: 0.4830 - dense_87_loss: 0.3838 - dense_90_loss: 0.0992 - dense_87_acc: 0.8656 - dense_90_acc: 0.9656\n",
      "Epoch 149/200\n",
      " - 17s - loss: 0.4874 - dense_87_loss: 0.3834 - dense_90_loss: 0.1041 - dense_87_acc: 0.8682 - dense_90_acc: 0.9638\n",
      "Epoch 150/200\n",
      " - 17s - loss: 0.4863 - dense_87_loss: 0.3833 - dense_90_loss: 0.1030 - dense_87_acc: 0.8670 - dense_90_acc: 0.9643\n",
      "Epoch 151/200\n",
      " - 17s - loss: 0.4771 - dense_87_loss: 0.3764 - dense_90_loss: 0.1007 - dense_87_acc: 0.8692 - dense_90_acc: 0.9645\n",
      "Epoch 152/200\n",
      " - 17s - loss: 0.4775 - dense_87_loss: 0.3800 - dense_90_loss: 0.0975 - dense_87_acc: 0.8679 - dense_90_acc: 0.9661\n",
      "Epoch 153/200\n",
      " - 17s - loss: 0.4719 - dense_87_loss: 0.3735 - dense_90_loss: 0.0984 - dense_87_acc: 0.8713 - dense_90_acc: 0.9670\n",
      "Epoch 154/200\n",
      " - 17s - loss: 0.4725 - dense_87_loss: 0.3730 - dense_90_loss: 0.0994 - dense_87_acc: 0.8708 - dense_90_acc: 0.9662\n",
      "Epoch 155/200\n",
      " - 17s - loss: 0.4647 - dense_87_loss: 0.3704 - dense_90_loss: 0.0943 - dense_87_acc: 0.8721 - dense_90_acc: 0.9671\n",
      "Epoch 156/200\n",
      " - 17s - loss: 0.4700 - dense_87_loss: 0.3727 - dense_90_loss: 0.0973 - dense_87_acc: 0.8687 - dense_90_acc: 0.9667\n",
      "Epoch 157/200\n",
      " - 17s - loss: 0.4658 - dense_87_loss: 0.3682 - dense_90_loss: 0.0977 - dense_87_acc: 0.8727 - dense_90_acc: 0.9674\n",
      "Epoch 158/200\n",
      " - 17s - loss: 0.4663 - dense_87_loss: 0.3700 - dense_90_loss: 0.0963 - dense_87_acc: 0.8707 - dense_90_acc: 0.9675\n",
      "Epoch 159/200\n",
      " - 17s - loss: 0.4638 - dense_87_loss: 0.3698 - dense_90_loss: 0.0940 - dense_87_acc: 0.8714 - dense_90_acc: 0.9679\n",
      "Epoch 160/200\n",
      " - 17s - loss: 0.4614 - dense_87_loss: 0.3659 - dense_90_loss: 0.0955 - dense_87_acc: 0.8725 - dense_90_acc: 0.9678\n",
      "Epoch 161/200\n",
      " - 17s - loss: 0.4572 - dense_87_loss: 0.3656 - dense_90_loss: 0.0916 - dense_87_acc: 0.8728 - dense_90_acc: 0.9677\n",
      "Epoch 162/200\n",
      " - 17s - loss: 0.4588 - dense_87_loss: 0.3679 - dense_90_loss: 0.0909 - dense_87_acc: 0.8716 - dense_90_acc: 0.9694\n",
      "Epoch 163/200\n",
      " - 17s - loss: 0.4535 - dense_87_loss: 0.3626 - dense_90_loss: 0.0910 - dense_87_acc: 0.8720 - dense_90_acc: 0.9682\n",
      "Epoch 164/200\n",
      " - 17s - loss: 0.4496 - dense_87_loss: 0.3603 - dense_90_loss: 0.0893 - dense_87_acc: 0.8748 - dense_90_acc: 0.9696\n",
      "Epoch 165/200\n",
      " - 17s - loss: 0.4504 - dense_87_loss: 0.3590 - dense_90_loss: 0.0915 - dense_87_acc: 0.8741 - dense_90_acc: 0.9691\n",
      "Epoch 166/200\n",
      " - 17s - loss: 0.4468 - dense_87_loss: 0.3588 - dense_90_loss: 0.0880 - dense_87_acc: 0.8748 - dense_90_acc: 0.9703\n",
      "Epoch 167/200\n",
      " - 17s - loss: 0.4469 - dense_87_loss: 0.3584 - dense_90_loss: 0.0885 - dense_87_acc: 0.8755 - dense_90_acc: 0.9696\n",
      "Epoch 168/200\n",
      " - 17s - loss: 0.4458 - dense_87_loss: 0.3570 - dense_90_loss: 0.0888 - dense_87_acc: 0.8769 - dense_90_acc: 0.9698\n",
      "Epoch 169/200\n",
      " - 17s - loss: 0.4457 - dense_87_loss: 0.3556 - dense_90_loss: 0.0900 - dense_87_acc: 0.8760 - dense_90_acc: 0.9697\n",
      "Epoch 170/200\n",
      " - 17s - loss: 0.4418 - dense_87_loss: 0.3542 - dense_90_loss: 0.0876 - dense_87_acc: 0.8759 - dense_90_acc: 0.9694\n",
      "Epoch 171/200\n",
      " - 17s - loss: 0.4409 - dense_87_loss: 0.3579 - dense_90_loss: 0.0830 - dense_87_acc: 0.8752 - dense_90_acc: 0.9725\n",
      "Epoch 172/200\n",
      " - 17s - loss: 0.4399 - dense_87_loss: 0.3529 - dense_90_loss: 0.0870 - dense_87_acc: 0.8776 - dense_90_acc: 0.9710\n",
      "Epoch 173/200\n",
      " - 17s - loss: 0.4289 - dense_87_loss: 0.3458 - dense_90_loss: 0.0832 - dense_87_acc: 0.8784 - dense_90_acc: 0.9718\n",
      "Epoch 174/200\n",
      " - 17s - loss: 0.4338 - dense_87_loss: 0.3491 - dense_90_loss: 0.0847 - dense_87_acc: 0.8775 - dense_90_acc: 0.9714\n",
      "Epoch 175/200\n",
      " - 17s - loss: 0.4345 - dense_87_loss: 0.3466 - dense_90_loss: 0.0878 - dense_87_acc: 0.8791 - dense_90_acc: 0.9705\n",
      "Epoch 176/200\n",
      " - 17s - loss: 0.4291 - dense_87_loss: 0.3485 - dense_90_loss: 0.0806 - dense_87_acc: 0.8789 - dense_90_acc: 0.9723\n",
      "Epoch 177/200\n",
      " - 17s - loss: 0.4252 - dense_87_loss: 0.3420 - dense_90_loss: 0.0832 - dense_87_acc: 0.8810 - dense_90_acc: 0.9717\n",
      "Epoch 178/200\n",
      " - 17s - loss: 0.4232 - dense_87_loss: 0.3419 - dense_90_loss: 0.0814 - dense_87_acc: 0.8803 - dense_90_acc: 0.9727\n",
      "Epoch 179/200\n",
      " - 17s - loss: 0.4213 - dense_87_loss: 0.3402 - dense_90_loss: 0.0811 - dense_87_acc: 0.8827 - dense_90_acc: 0.9725\n",
      "Epoch 180/200\n",
      " - 17s - loss: 0.4194 - dense_87_loss: 0.3374 - dense_90_loss: 0.0820 - dense_87_acc: 0.8810 - dense_90_acc: 0.9730\n",
      "Epoch 181/200\n",
      " - 17s - loss: 0.4263 - dense_87_loss: 0.3465 - dense_90_loss: 0.0797 - dense_87_acc: 0.8801 - dense_90_acc: 0.9735\n",
      "Epoch 182/200\n",
      " - 17s - loss: 0.4218 - dense_87_loss: 0.3405 - dense_90_loss: 0.0813 - dense_87_acc: 0.8818 - dense_90_acc: 0.9730\n",
      "Epoch 183/200\n",
      " - 17s - loss: 0.4206 - dense_87_loss: 0.3410 - dense_90_loss: 0.0796 - dense_87_acc: 0.8799 - dense_90_acc: 0.9734\n",
      "Epoch 184/200\n",
      " - 17s - loss: 0.4155 - dense_87_loss: 0.3368 - dense_90_loss: 0.0787 - dense_87_acc: 0.8819 - dense_90_acc: 0.9732\n",
      "Epoch 185/200\n",
      " - 17s - loss: 0.4129 - dense_87_loss: 0.3343 - dense_90_loss: 0.0786 - dense_87_acc: 0.8829 - dense_90_acc: 0.9739\n",
      "Epoch 186/200\n",
      " - 17s - loss: 0.4159 - dense_87_loss: 0.3353 - dense_90_loss: 0.0806 - dense_87_acc: 0.8817 - dense_90_acc: 0.9728\n",
      "Epoch 187/200\n",
      " - 17s - loss: 0.4140 - dense_87_loss: 0.3368 - dense_90_loss: 0.0771 - dense_87_acc: 0.8805 - dense_90_acc: 0.9740\n",
      "Epoch 188/200\n",
      " - 17s - loss: 0.4091 - dense_87_loss: 0.3336 - dense_90_loss: 0.0756 - dense_87_acc: 0.8847 - dense_90_acc: 0.9745\n",
      "Epoch 189/200\n",
      " - 17s - loss: 0.4077 - dense_87_loss: 0.3324 - dense_90_loss: 0.0753 - dense_87_acc: 0.8826 - dense_90_acc: 0.9747\n",
      "Epoch 190/200\n",
      " - 17s - loss: 0.4027 - dense_87_loss: 0.3295 - dense_90_loss: 0.0732 - dense_87_acc: 0.8835 - dense_90_acc: 0.9751\n",
      "Epoch 191/200\n",
      " - 17s - loss: 0.4094 - dense_87_loss: 0.3310 - dense_90_loss: 0.0783 - dense_87_acc: 0.8848 - dense_90_acc: 0.9737\n",
      "Epoch 192/200\n",
      " - 17s - loss: 0.4083 - dense_87_loss: 0.3312 - dense_90_loss: 0.0771 - dense_87_acc: 0.8845 - dense_90_acc: 0.9737\n",
      "Epoch 193/200\n",
      " - 17s - loss: 0.4049 - dense_87_loss: 0.3303 - dense_90_loss: 0.0746 - dense_87_acc: 0.8833 - dense_90_acc: 0.9744\n",
      "Epoch 194/200\n",
      " - 17s - loss: 0.4038 - dense_87_loss: 0.3316 - dense_90_loss: 0.0722 - dense_87_acc: 0.8845 - dense_90_acc: 0.9756\n",
      "Epoch 195/200\n",
      " - 17s - loss: 0.3997 - dense_87_loss: 0.3273 - dense_90_loss: 0.0723 - dense_87_acc: 0.8853 - dense_90_acc: 0.9757\n",
      "Epoch 196/200\n",
      " - 17s - loss: 0.3941 - dense_87_loss: 0.3228 - dense_90_loss: 0.0713 - dense_87_acc: 0.8874 - dense_90_acc: 0.9765\n",
      "Epoch 197/200\n",
      " - 17s - loss: 0.3971 - dense_87_loss: 0.3242 - dense_90_loss: 0.0730 - dense_87_acc: 0.8862 - dense_90_acc: 0.9758\n",
      "Epoch 198/200\n",
      " - 17s - loss: 0.3907 - dense_87_loss: 0.3203 - dense_90_loss: 0.0704 - dense_87_acc: 0.8891 - dense_90_acc: 0.9761\n",
      "Epoch 199/200\n",
      " - 17s - loss: 0.3954 - dense_87_loss: 0.3219 - dense_90_loss: 0.0735 - dense_87_acc: 0.8870 - dense_90_acc: 0.9751\n",
      "Epoch 200/200\n",
      " - 17s - loss: 0.3884 - dense_87_loss: 0.3188 - dense_90_loss: 0.0696 - dense_87_acc: 0.8876 - dense_90_acc: 0.9766\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "batch_size = 200\n",
    "# history = model.fit_generator(datagen.flow(x_train, y_train_vec, batch_size=batch_size), steps_per_epoch=x_train.shape[0]//batch_size,epochs=200, verbose=2)\n",
    "history = model.fit_generator(batch_generator(x_train,y_train_vec,batch_size=batch_size), steps_per_epoch=x_train.shape[0]//batch_size,epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\r   32/10000 [..............................] - ETA: 2:26",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  224/10000 [..............................] - ETA: 22s ",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  416/10000 [>.............................] - ETA: 13s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  608/10000 [>.............................] - ETA: 9s ",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  832/10000 [=>............................] - ETA: 7s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1024/10000 [==>...........................] - ETA: 6s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1216/10000 [==>...........................] - ETA: 5s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1408/10000 [===>..........................] - ETA: 5s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1600/10000 [===>..........................] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1824/10000 [====>.........................] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2016/10000 [=====>........................] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2208/10000 [=====>........................] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2400/10000 [======>.......................] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2592/10000 [======>.......................] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2816/10000 [=======>......................] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3008/10000 [========>.....................] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3232/10000 [========>.....................] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3424/10000 [=========>....................] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3616/10000 [=========>....................] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3808/10000 [==========>...................] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4000/10000 [===========>..................] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4224/10000 [===========>..................] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4416/10000 [============>.................] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4608/10000 [============>.................] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4800/10000 [=============>................] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5024/10000 [==============>...............] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5216/10000 [==============>...............] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5408/10000 [===============>..............] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5600/10000 [===============>..............] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5792/10000 [================>.............] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5984/10000 [================>.............] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6176/10000 [=================>............] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6400/10000 [==================>...........] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6624/10000 [==================>...........] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6848/10000 [===================>..........] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7040/10000 [====================>.........] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7200/10000 [====================>.........] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7424/10000 [=====================>........] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7648/10000 [=====================>........] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7840/10000 [======================>.......] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8064/10000 [=======================>......] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8288/10000 [=======================>......] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8480/10000 [========================>.....] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8704/10000 [=========================>....] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8928/10000 [=========================>....] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9152/10000 [==========================>...] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9344/10000 [===========================>..] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9536/10000 [===========================>..] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9728/10000 [============================>.] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9952/10000 [============================>.] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10000/10000 [==============================]",
      " - 3s 312us/step\n",
      "loss = 0.5042510032653809\naccuracy = 0.833899974822998\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, [y_test_vec,y_test_vec])\n",
    "print('loss = ' + str(loss_and_acc[1]))\n",
    "print('accuracy = ' + str(loss_and_acc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This application is used to convert notebook files (*.ipynb) to various other\n",
      "formats.\n",
      "\n",
      "WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
      "\n",
      "Options\n",
      "\n",
      "-------\n",
      "\n",
      "\n",
      "\n",
      "Arguments that take values are actually convenience aliases to full\n",
      "Configurables, whose aliases are listed on the help line. For more information\n",
      "on full configurables, see '--help-all'.\n",
      "\n",
      "\n",
      "--debug\n",
      "\n",
      "    set log level to logging.DEBUG (maximize logging output)\n",
      "\n",
      "--generate-config\n",
      "\n",
      "    generate default config file\n",
      "\n",
      "-y\n",
      "\n",
      "    Answer yes to any questions instead of prompting.\n",
      "\n",
      "--execute\n",
      "\n",
      "    Execute the notebook prior to export.\n",
      "\n",
      "--allow-errors\n",
      "\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
      "\n",
      "--stdin\n",
      "\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
      "\n",
      "--stdout\n",
      "\n",
      "    Write notebook output to stdout instead of files.\n",
      "\n",
      "--inplace\n",
      "\n",
      "    Run nbconvert in place, overwriting the existing notebook (only \n",
      "    relevant when converting to notebook format)\n",
      "\n",
      "--clear-output\n",
      "\n",
      "    Clear output of current file and save in place, \n",
      "    overwriting the existing notebook.\n",
      "\n",
      "--no-prompt\n",
      "\n",
      "    Exclude input and output prompts from converted document.\n",
      "\n",
      "--no-input\n",
      "\n",
      "    Exclude input cells and output prompts from converted document. \n",
      "    This mode is ideal for generating code-free reports.\n",
      "--log-level=<Enum> (Application.log_level)\n",
      "\n",
      "    Default: 30\n",
      "\n",
      "    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\n",
      "\n",
      "    Set the log level by value or name.\n",
      "\n",
      "--config=<Unicode> (JupyterApp.config_file)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    Full path of a config file.\n",
      "\n",
      "--to=<Unicode> (NbConvertApp.export_format)\n",
      "\n",
      "    Default: 'html'\n",
      "\n",
      "    The export format to be used, either one of the built-in formats\n",
      "\n",
      "    ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf',\n",
      "\n",
      "    'python', 'rst', 'script', 'slides'] or a dotted object name that represents\n",
      "\n",
      "    the import path for an `Exporter` class\n",
      "\n",
      "--template=<Unicode> (TemplateExporter.template_file)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    Name of the template file to use\n",
      "\n",
      "--writer=<DottedObjectName> (NbConvertApp.writer_class)\n",
      "\n",
      "    Default: 'FilesWriter'\n",
      "\n",
      "    Writer class used to write the  results of the conversion\n",
      "\n",
      "--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    PostProcessor class used to write the results of the conversion\n",
      "\n",
      "--output=<Unicode> (NbConvertApp.output_base)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    overwrite base name use for output files. can only be used when converting\n",
      "\n",
      "    one notebook at a time.\n",
      "\n",
      "--output-dir=<Unicode> (FilesWriter.build_directory)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    Directory to write output(s) to. Defaults to output to the directory of each\n",
      "\n",
      "    notebook. To recover previous default behaviour (outputting to the current\n",
      "\n",
      "    working directory) use . as the flag value.\n",
      "\n",
      "--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\n",
      "\n",
      "    Default: ''\n",
      "\n",
      "    The URL prefix for reveal.js (version 3.x). This defaults to the reveal CDN,\n",
      "\n",
      "    but can be any url pointing to a copy  of reveal.js.\n",
      "\n",
      "    For speaker notes to work, this must be a relative path to a local  copy of\n",
      "\n",
      "    reveal.js: e.g., \"reveal.js\".\n",
      "\n",
      "    If a relative path is given, it must be a subdirectory of the current\n",
      "\n",
      "    directory (from which the server is run).\n",
      "\n",
      "    See the usage documentation\n",
      "\n",
      "    (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-\n",
      "\n",
      "    slideshow) for more details.\n",
      "\n",
      "--nbformat=<Enum> (NotebookExporter.nbformat_version)\n",
      "\n",
      "    Default: 4\n",
      "\n",
      "    Choices: [1, 2, 3, 4]\n",
      "\n",
      "    The nbformat version to write. Use this to downgrade notebooks.\n",
      "\n",
      "To see all available configurables, use `--help-all`\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "    The simplest way to use nbconvert is\n",
      "    \n",
      "    > jupyter nbconvert mynotebook.ipynb\n",
      "    \n",
      "    which will convert mynotebook.ipynb to the default format (probably HTML).\n",
      "    \n",
      "    You can specify the export format with `--to`.\n",
      "    Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n",
      "    \n",
      "    > jupyter nbconvert --to latex mynotebook.ipynb\n",
      "    \n",
      "    Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
      "    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n",
      "    can specify the flavor of the format used.\n",
      "    \n",
      "    > jupyter nbconvert --to html --template basic mynotebook.ipynb\n",
      "    \n",
      "    You can also pipe the output to stdout, rather than a file\n",
      "    \n",
      "    > jupyter nbconvert mynotebook.ipynb --stdout\n",
      "    \n",
      "    PDF is generated via latex\n",
      "    \n",
      "    > jupyter nbconvert mynotebook.ipynb --to pdf\n",
      "    \n",
      "    You can get (and serve) a Reveal.js-powered slideshow\n",
      "    \n",
      "    > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
      "    \n",
      "    Multiple notebooks can be given at the command line in a couple of \n",
      "    different ways:\n",
      "    \n",
      "    > jupyter nbconvert notebook*.ipynb\n",
      "    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
      "    \n",
      "    or you can specify the notebooks list in a config file, containing::\n",
      "    \n",
      "        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
      "    \n",
      "    > jupyter nbconvert --config mycfg.py\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | pattern 'HM4.ipynb' matched no files\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html HM4.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}